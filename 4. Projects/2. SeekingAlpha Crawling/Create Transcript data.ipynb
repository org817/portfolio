{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "from collections import Counter\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Index files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = 'C:/Users/Rocku/Downloads/SeekingAlpha_EarningCalls/full_indexed_utf8.csv'\n",
    "df = pd.read_csv(path)\n",
    "df_ex = df.loc[df['PositionIndex2'] == 1]\n",
    "df_ex.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load full index file (Executives + Analysts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path2 = 'C:/Users/Rocku/Downloads/SeekingAlpha_EarningCalls/index_file2.csv'\n",
    "df_full_index = pd.read_csv(path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_ex_example = df_ex[300000:]\n",
    "total = df_ex_example.count()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_words(indi_df, qna_index):\n",
    "    regex1 = re.compile('[.,?!]')\n",
    "\n",
    "    transcript_dict= {}\n",
    "    \n",
    "    indi_list = []\n",
    "    for i, transcript in indi_df.iterrows():\n",
    "        indi_list.append(str(transcript['Transcripts']))\n",
    "    \n",
    "    indi = '. '.join(indi_list)\n",
    "    indi = indi.replace('..', '.').replace('\\n', '.')\n",
    "    splitted_transcript = regex1.split(indi.strip())[:-1]    \n",
    "    \n",
    "    wordcount = Counter(indi.replace(',',' ').replace('.',' ')\n",
    "                        .replace('!',' ').replace('?',' ').lower().split())\n",
    "    \n",
    "    word_a = wordcount['a'] + wordcount['an']\n",
    "    word_can = wordcount['can']\n",
    "    word_could = wordcount['could'] \n",
    "    word_may = wordcount['may']\n",
    "    word_might = wordcount['might']\n",
    "    word_must = wordcount['must']\n",
    "    word_shall = wordcount['shall']\n",
    "    word_will = wordcount['will']\n",
    "    word_would = wordcount['would']\n",
    "    \n",
    "    if len(splitted_transcript) != 0:\n",
    "        transcript_dict['N_WPerS_%s'%qna_index] = round(len(wordcount)/\n",
    "                                                  len(splitted_transcript), 3)\n",
    "    else:\n",
    "        transcript_dict['N_WPerS_%s'%qna_index] = 0\n",
    "        \n",
    "    transcript_dict['N_A_%s'%qna_index] = wordcount['a'] + wordcount['an']\n",
    "    transcript_dict['N_The_%s'%qna_index] = wordcount['the']\n",
    "    transcript_dict['N_Can_%s'%qna_index] = wordcount['can']\n",
    "    transcript_dict['N_Could_%s'%qna_index] = wordcount['could'] \n",
    "    transcript_dict['N_May_%s'%qna_index] = wordcount['may']\n",
    "    transcript_dict['N_Might_%s'%qna_index] = wordcount['might']\n",
    "    transcript_dict['N_Must_%s'%qna_index] = wordcount['must']\n",
    "    transcript_dict['N_Shall_%s'%qna_index] = wordcount['shall']\n",
    "    transcript_dict['N_Will_%s'%qna_index] = wordcount['will']\n",
    "    transcript_dict['N_Would_%s'%qna_index] = wordcount['would']\n",
    "    \n",
    "    transcript_dict['Transcript_%s'%qna_index] = indi\n",
    "    transcript_dict['N_Words_%s'%qna_index] = len(wordcount)\n",
    "    transcript_dict['N_Sent_%s'%qna_index] = len(splitted_transcript)\n",
    "    transcript_dict['N_Par_%s'%qna_index] = indi_df.count()[0]\n",
    "    \n",
    "    del indi_list, indi_df, indi, wordcount, splitted_transcript\n",
    "    \n",
    "    return transcript_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check_talker(filename, talkers):\n",
    "    data = df_full_index.loc[df_full_index['FileName'] == filename]\n",
    "    data = data['Name'].tolist()\n",
    "    \n",
    "    not_in_lists = []\n",
    "    for talker in talkers:\n",
    "        talker = talker.lower().strip()\n",
    "        if ((talker.startswith('quest')) or (talker.startswith('operat')):\n",
    "            pass\n",
    "        else:\n",
    "            if (len(talker.split()) < 4) & (talker not in data):\n",
    "                not_in_lists.append(talker)\n",
    "    \n",
    "    del data, talkers\n",
    "    \n",
    "    return not_in_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns = ['URL', 'Title', 'UploadDate', 'CompanyName', 'Exchange', 'Ticker', \n",
    "           'FileIndex', 'NameIndex','PositionIndex1',\n",
    "           'Subtitle', 'CallDate', 'Executive', \n",
    "           'N_Sent_All', 'N_Sent_BeforeQ','N_Sent_AfterQ', \n",
    "           'N_Par_All', 'N_Par_BeforeQ','N_Par_AfterQ',\n",
    "           'N_WPerS_All', 'N_WPerS_BeforeQ','N_WPerS_AfterQ', \n",
    "           'N_Can_All', 'N_Can_BeforeQ','N_Can_AfterQ', \n",
    "           'N_Could_All', 'N_Could_BeforeQ','N_Could_AfterQ', \n",
    "           'N_May_All', 'N_May_BeforeQ', 'N_May_AfterQ', \n",
    "           'N_Might_All', 'N_Might_BeforeQ','N_Might_AfterQ', \n",
    "           'N_Must_All', 'N_Must_BeforeQ','N_Must_AfterQ', \n",
    "           'N_Shall_All', 'N_Shall_BeforeQ','N_Shall_AfterQ', \n",
    "           'N_Will_All', 'N_Will_BeforeQ','N_Will_AfterQ', \n",
    "           'N_A_All', 'N_A_BeforeQ','N_A_AfterQ', \n",
    "           'N_The_All', 'N_The_BeforeQ','N_The_AfterQ',\n",
    "           'N_Words_All', 'N_Words_BeforeQ','N_Words_AfterQ'\n",
    "          ]\n",
    "df_output = pd.DataFrame(columns=columns)\n",
    "\n",
    "columns_not_in_lits = ['FileIndex', 'FileName', 'NotInList']\n",
    "\n",
    "df_not_in_list_table = pd.DataFrame(columns=columns_not_in_lits)\n",
    "\n",
    "failed = set()\n",
    "next_time = set()\n",
    "successed = set()\n",
    "\n",
    "total = df_ex.count()[0]\n",
    "\n",
    "prev_path = None\n",
    "# for i, row in df_ex.iterrows():\n",
    "for i, row in df_ex_example.iterrows():\n",
    "    if (i%100 == 1):\n",
    "        print(i, '/', total, row['FileName'], 'proceeding:', datetime.datetime.now())\n",
    "    filename = row['FileName']\n",
    "    \n",
    "    try:\n",
    "        file_path = 'C:/Users/Rocku/Downloads/SeekingAlpha_EarningCalls/raw_xlsx/' + filename\n",
    "        if (prev_path is not None) and (prev_path == file_path):\n",
    "            df_temp = df_temp\n",
    "        else:   \n",
    "            df_temp = pd.read_excel(file_path, header=None, index_col=0, encoding='utf-8')\n",
    "            prev_path = file_path\n",
    "\n",
    "        # Get Transcript text\n",
    "        df_trans = df_temp.loc['Transcript']\n",
    "        df_trans = df_trans.reset_index(drop=True)\n",
    "        df_trans.rename(columns={1:'Name', 2:'Transcripts'}, inplace=True)\n",
    "\n",
    "        # Find talkers in transcript who are not in the Executives and Analysts list\n",
    "        df_trans.loc[:,'Name'] = df_trans.loc[:,'Name'].str.split(r'\\s\\W+\\s').str[0]\n",
    "        talkers = list(set(df_trans['Name'].tolist()))\n",
    "        not_in_list = check_talker(filename=filename, talkers = talkers)\n",
    "        \n",
    "        if (len(not_in_list) == 0):\n",
    "            # Find QnA Index to divide into two call sessions\n",
    "            try:\n",
    "                try:\n",
    "                    qna_index = df_trans.loc[df_trans['Name'] == 'Question-and-Answer Session'].index[0]\n",
    "                except:\n",
    "                    qna_index = df_trans.loc[df_trans['Name'] == 'Question and Answer Session'].index[0]\n",
    "\n",
    "                df_trans_before_qna = df_trans.loc[:qna_index-1]\n",
    "                df_trans_after_qna = df_trans.loc[qna_index+1:]\n",
    "\n",
    "            except:\n",
    "                df_trans_before_qna = df_trans\n",
    "                df_trans_after_qna = df_trans.loc[:0]\n",
    "\n",
    "            indi_after = df_trans_after_qna.loc[df_trans_after_qna['Name'] ==  row['Name']]\n",
    "            indi_before = df_trans_before_qna.loc[df_trans_before_qna['Name'] ==  row['Name']]\n",
    "            \n",
    "            # Word count\n",
    "            transcript_dict_Before = count_words(indi_before, 'BeforeQ')\n",
    "            transcript_dict_After = count_words(indi_after, 'AfterQ')\n",
    "\n",
    "            try:\n",
    "                Title = df_temp.loc['Title'][1]\n",
    "            except:\n",
    "                Title = 'None'\n",
    "\n",
    "            try: \n",
    "                Subtitle = df_temp.loc['Subtitle'][1]\n",
    "                if Subtitle == 'Executives':\n",
    "                    Subtitle = 'None'\n",
    "            except:\n",
    "                Subtitle = 'None'\n",
    "\n",
    "            row_dict = {}\n",
    "\n",
    "            row_dict['CompanyName'] = row['CompanyName']\n",
    "            row_dict['Exchange'] = row['Exchange']\n",
    "            row_dict['Ticker'] = row['Ticker']\n",
    "            row_dict['FileIndex'] = row['FileIndex']\n",
    "            row_dict['UploadDate'] = row['UploadDate']\n",
    "            row_dict['NameIndex'] = row['NameIndex']\n",
    "            row_dict['PositionIndex1'] = row['PositionIndex1']\n",
    "            row_dict['CallDate'] = row['CallDate']\n",
    "            row_dict['Executive'] = str(row['Name']) + '-' + str(row['OriginalPosition'])\n",
    "            row_dict['URL'] = 'https://seekingalpha.com/article/' + row['FileName'].replace('_', '.').split('.')[1]\n",
    "            row_dict['Title']  = Title\n",
    "            row_dict['Subtitle'] = Subtitle\n",
    "\n",
    "            row_dict['N_Sent_BeforeQ'] = transcript_dict_Before['N_Sent_BeforeQ']\n",
    "            row_dict['N_Sent_AfterQ'] = transcript_dict_After['N_Sent_AfterQ']\n",
    "            row_dict['N_Sent_All'] = row_dict['N_Sent_BeforeQ'] + row_dict['N_Sent_AfterQ']\n",
    "\n",
    "            row_dict['N_WPerS_BeforeQ'] = transcript_dict_Before['N_WPerS_BeforeQ']\n",
    "            row_dict['N_WPerS_AfterQ'] = transcript_dict_After['N_WPerS_AfterQ']\n",
    "            if (row_dict['N_Sent_All'] == 0):\n",
    "                row_dict['N_WPerS_All'] = 0\n",
    "            else:\n",
    "                row_dict['N_WPerS_All'] = round((row_dict['N_WPerS_BeforeQ']*row_dict['N_Sent_BeforeQ']+row_dict['N_WPerS_AfterQ']*row_dict['N_Sent_AfterQ'])/(row_dict['N_Sent_All']), 3)\n",
    "\n",
    "            row_dict['N_Can_BeforeQ'] = transcript_dict_Before['N_Can_BeforeQ']\n",
    "            row_dict['N_Can_AfterQ'] = transcript_dict_After['N_Can_AfterQ']\n",
    "            row_dict['N_Can_All'] = row_dict['N_Can_BeforeQ'] + row_dict['N_Can_AfterQ']\n",
    "\n",
    "            row_dict['N_Could_BeforeQ'] = transcript_dict_Before['N_Could_BeforeQ']\n",
    "            row_dict['N_Could_AfterQ'] = transcript_dict_After['N_Could_AfterQ']\n",
    "            row_dict['N_Could_All'] = row_dict['N_Could_BeforeQ'] + row_dict['N_Could_AfterQ']\n",
    "\n",
    "            row_dict['N_May_BeforeQ'] = transcript_dict_Before['N_May_BeforeQ']\n",
    "            row_dict['N_May_AfterQ'] = transcript_dict_After['N_May_AfterQ']\n",
    "            row_dict['N_May_All'] = row_dict['N_May_BeforeQ'] + row_dict['N_May_AfterQ']\n",
    "\n",
    "            row_dict['N_Might_BeforeQ'] = transcript_dict_Before['N_Might_BeforeQ']\n",
    "            row_dict['N_Might_AfterQ'] = transcript_dict_After['N_Might_AfterQ']\n",
    "            row_dict['N_Might_All'] = row_dict['N_Might_BeforeQ'] + row_dict['N_Might_AfterQ']\n",
    "\n",
    "            row_dict['N_Must_BeforeQ'] = transcript_dict_Before['N_Must_BeforeQ']\n",
    "            row_dict['N_Must_AfterQ'] = transcript_dict_After['N_Must_AfterQ']\n",
    "            row_dict['N_Must_All'] = row_dict['N_Must_BeforeQ'] + row_dict['N_Must_AfterQ']\n",
    "\n",
    "            row_dict['N_Shall_BeforeQ'] = transcript_dict_Before['N_Shall_BeforeQ']\n",
    "            row_dict['N_Shall_AfterQ'] = transcript_dict_After['N_Shall_AfterQ']\n",
    "            row_dict['N_Shall_All'] = row_dict['N_Shall_BeforeQ'] + row_dict['N_Shall_AfterQ']\n",
    "\n",
    "            row_dict['N_Will_BeforeQ'] = transcript_dict_Before['N_Will_BeforeQ']\n",
    "            row_dict['N_Will_AfterQ'] = transcript_dict_After['N_Will_AfterQ']\n",
    "            row_dict['N_Will_All'] = row_dict['N_Will_BeforeQ'] + row_dict['N_Will_AfterQ']\n",
    "\n",
    "            row_dict['N_A_BeforeQ'] = transcript_dict_Before['N_A_BeforeQ']\n",
    "            row_dict['N_A_AfterQ'] = transcript_dict_After['N_A_AfterQ']\n",
    "            row_dict['N_A_All'] = row_dict['N_A_BeforeQ'] + row_dict['N_A_AfterQ']\n",
    "\n",
    "            row_dict['N_The_BeforeQ'] = transcript_dict_Before['N_The_BeforeQ']\n",
    "            row_dict['N_The_AfterQ'] = transcript_dict_After['N_The_AfterQ']\n",
    "            row_dict['N_The_All'] = row_dict['N_The_BeforeQ'] + row_dict['N_The_AfterQ']\n",
    "\n",
    "            row_dict['N_Words_BeforeQ'] = transcript_dict_Before['N_Words_BeforeQ']\n",
    "            row_dict['N_Words_AfterQ'] = transcript_dict_After['N_Words_AfterQ']\n",
    "            row_dict['N_Words_All'] = row_dict['N_Words_BeforeQ'] + row_dict['N_Words_AfterQ']\n",
    "            \n",
    "            row_dict['N_Par_BeforeQ'] = transcript_dict_Before['N_Par_BeforeQ']\n",
    "            row_dict['N_Par_AfterQ'] = transcript_dict_After['N_Par_AfterQ']\n",
    "            row_dict['N_Par_All'] = row_dict['N_Par_BeforeQ'] + row_dict['N_Par_AfterQ']\n",
    "            \n",
    "            \n",
    "            df_new = pd.DataFrame([row_dict], columns=columns)\n",
    "            df_output = df_output.append(df_new)\n",
    "            \n",
    "            del df_new, indi_after, indi_before, transcript_dict_Before, transcript_dict_After\n",
    "            successed.add(filename)\n",
    "            \n",
    "        else:\n",
    "            not_in_lists_dict = {}\n",
    "            \n",
    "            if (row['FileIndex'] not in df_not_in_list_table['FileIndex'].tolist()):\n",
    "                for name in not_in_list:\n",
    "                    not_in = {}\n",
    "                    not_in_lists_dict['FileName'] = row['FileName']\n",
    "                    not_in_lists_dict['FileIndex'] = row['FileIndex']\n",
    "                    not_in_lists_dict['NotInList'] = name\n",
    "\n",
    "                    df_not_in_lists = pd.DataFrame([not_in_lists_dict], columns=columns_not_in_lits)\n",
    "                    df_not_in_list_table = df_not_in_list_table.append(df_not_in_lists)\n",
    "                    del df_not_in_lists\n",
    "                del not_in_lists_dict\n",
    "            \n",
    "            next_time.add(filename)\n",
    "            \n",
    "    except:\n",
    "        failed.add(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_output.to_csv('word_count_by_index_4.csv', header='column_names', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_not_in_list_table.to_csv('not_in_lists_4.csv', header='column_names', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
