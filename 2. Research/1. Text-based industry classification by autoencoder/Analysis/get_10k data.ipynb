{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# https://www.sec.gov/Archives/edgar/full-index/2017/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import lxml\n",
    "import re\n",
    "import os\n",
    "\n",
    "proxy_list = pd.read_table('proxy.txt',sep='\\t', header=-1)\n",
    "header_list = pd.read_table('header.txt',sep='\\t', header=-1)\n",
    "\n",
    "def reset_proxy():\n",
    "    rand_sample = random.sample(range(len(proxy_list)), 1)\n",
    "\n",
    "    rand_ip = proxy_list.ix[rand_sample][0].values\n",
    "    rand_port = proxy_list.ix[rand_sample][1].values\n",
    "\n",
    "    proxy_target = (str(*rand_ip) +':'+  str(*rand_port))\n",
    "    \n",
    "    return proxy_target\n",
    "\n",
    "\n",
    "def reset_header():\n",
    "    rand_sample = random.sample(range(len(header_list)), 1)\n",
    "\n",
    "    rand_header = header_list.ix[rand_sample][0].values\n",
    "\n",
    "    header_target = (str(*rand_header))\n",
    "    \n",
    "    return header_target\n",
    "\n",
    "\n",
    "def get_10k_path(year):\n",
    "#     qtrs = ['QTR1', 'QTR2', 'QTR3', 'QTR4']\n",
    "    qtrs = ['QTR4'] # Only consider 4th Querter of year\n",
    "    columns = ['CompanyName','FormType','CIK','Date','URL']\n",
    "    base_url = \"http://www.sec.gov/Archives/edgar/full-index/%s/\"%year\n",
    "    df_base = pd.DataFrame(columns=columns)\n",
    "    for qtr in qtrs:\n",
    "        url = base_url+qtr+'/crawler.idx'\n",
    "        print(url)\n",
    "        lines = requests.get(url).text.splitlines()\n",
    "        records=[]\n",
    "        for line in lines:\n",
    "            comnam = line[0:62].strip()\n",
    "            formt = line[62:74].strip()\n",
    "            cik = line[74:86].strip()\n",
    "            date = line[86:98].strip()\n",
    "            data_url = line[98:].strip()\n",
    "            records.append(tuple([comnam,formt,cik,date,data_url]))\n",
    "        df = pd.DataFrame(records, columns=columns)\n",
    "        df.to_excel('da.xlsx')\n",
    "        TenK = df[df['FormType'] == '10-K']\n",
    "        df_base = df_base.append(TenK)        \n",
    "        del df, TenK, records\n",
    "    print('Total number of 10-K filings : ',len(df_base))\n",
    "    return df_base\n",
    "\n",
    "cannot_crawl = []\n",
    "\n",
    "def down_10k(df):\n",
    "    df.reset_index(inplace=True)\n",
    "    base_url = \"http://www.sec.gov/Archives/\"\n",
    "    total_num = len(df)\n",
    "    for ind, rows in df.iterrows():\n",
    "        company = rows['CompanyName']\n",
    "        company = re.sub(r\"[\\\\+/+]\", \" \", company)\n",
    "        cik = rows['CIK']\n",
    "        date = rows['Date']\n",
    "#         url = base_url + rows['FileName']\n",
    "        url = rows['URL']\n",
    "        print(ind+1,'/',total_num, ' : ', url)\n",
    "        filename = '--'.join([cik,company,date]) + '.txt'\n",
    "        \n",
    "        flag = True\n",
    "        # try crawling three times if error occurs\n",
    "        while flag:\n",
    "            time.sleep(1)\n",
    "            flag_write = True\n",
    "            headers = {'User-Agent': reset_header()}\n",
    "            proxy_dict = {'http': reset_proxy()}\n",
    "            try:\n",
    "                response = requests.get(url, headers=headers, proxies=proxy_dict)\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    report = soup(response.content.decode() , 'lxml').get_text()\n",
    "                    report = re.sub(r\"\\s\\s+\", \" \", report)\n",
    "                    splitted = re.split(r'[iI]tem\\s+\\d+\\W+', report)\n",
    "                    for chunk in splitted:\n",
    "                        if (chunk.lower().strip().startswith('business') and len(chunk) > 100 and flag_write):\n",
    "                            with open('C:/Users/Rocku/downloads/sec_10k/'+filename, 'w', encoding='utf-8') as f:\n",
    "                                f.write(chunk)\n",
    "                            print(filename, ' : downloaded')\n",
    "                            flag_write=False\n",
    "\n",
    "                    flag = False\n",
    "            except:\n",
    "                with open('C:/Users/Rocku/downloads/sec_10k/'+'raw--'+filename, 'w', encoding='utf-8') as f:\n",
    "                    f.write(chunk)\n",
    "                print(filename, ' : raw downloaded')\n",
    "                cannot_crawl.append([company,cik,date,url])\n",
    "                flag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "directory = 'C:/Users/Rocku/downloads/sec_10k'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# Get urls of 10-K filings in 2017\n",
    "idx_data = get_10k_path(2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import lxml\n",
    "import re\n",
    "import os\n",
    "import concurrent.futures\n",
    "    \n",
    "proxy_list = pd.read_table('proxy.txt', sep='\\t', header=-1)\n",
    "header_list = pd.read_table('header.txt', sep='\\t', header=-1)\n",
    "\n",
    "\n",
    "def reset_proxy():\n",
    "    rand_sample = random.sample(range(len(proxy_list)), 1)\n",
    "\n",
    "    rand_ip = proxy_list.ix[rand_sample][0].values\n",
    "    rand_port = proxy_list.ix[rand_sample][1].values\n",
    "\n",
    "    proxy_target = (str(*rand_ip) + ':' + str(*rand_port))\n",
    "\n",
    "    return proxy_target\n",
    "\n",
    "\n",
    "def reset_header():\n",
    "    rand_sample = random.sample(range(len(header_list)), 1)\n",
    "\n",
    "    rand_header = header_list.ix[rand_sample][0].values\n",
    "\n",
    "    header_target = (str(*rand_header))\n",
    "\n",
    "    return header_target\n",
    "\n",
    "\n",
    "def get_10k_path(year):\n",
    "    #     qtrs = ['QTR1', 'QTR2', 'QTR3', 'QTR4']\n",
    "    qtrs = ['QTR4']  # Only consider 4th Querter of year\n",
    "    columns = ['CIK', 'CompanyName', 'FormType', 'Date', 'FileName']\n",
    "    base_url = \"http://www.sec.gov/Archives/edgar/full-index/%s/\" % year\n",
    "    df_base = pd.DataFrame(columns=columns)\n",
    "    for qtr in qtrs:\n",
    "        url = base_url + qtr + '/master.idx'\n",
    "        print(url)\n",
    "        lines = requests.get(url).text.splitlines()\n",
    "        records = [tuple(line.split('|')) for line in lines[11:]]\n",
    "        df = pd.DataFrame(records, columns=columns)\n",
    "        TenK = df[df['FormType'] == '10-K']\n",
    "        df_base = df_base.append(TenK)\n",
    "        del df, TenK, records\n",
    "    print('Total number of 10-K filings : ', len(df_base))\n",
    "    return df_base\n",
    "\n",
    "\n",
    "def download_processor(url, filename):\n",
    "    headers = {'User-Agent': reset_header()}\n",
    "    proxy_dict = {'http': reset_proxy()}\n",
    "    response = requests.get(url, headers=headers, proxies=proxy_dict)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        report = soup(response.content.decode(), 'lxml').get_text()\n",
    "        report = re.sub(r\"\\s\\s+\", \" \", report)\n",
    "        splitted = re.split(r'[iI]tem\\s+\\d+\\W+', report)\n",
    "        flag_write = True\n",
    "        for chunk in splitted:\n",
    "            if (chunk.lower().strip().startswith('business') and len(chunk) > 100 and flag_write):\n",
    "                with open('sec_10k/' + filename, 'w', encoding='utf-8') as f:\n",
    "                    f.write(chunk)\n",
    "                print(filename, ' : downloaded')\n",
    "                flag_write = False\n",
    "        del splitted, report\n",
    "    del response\n",
    "            \n",
    "\n",
    "def down_10k(df):\n",
    "    df.reset_index(inplace=True)\n",
    "    base_url = \"http://www.sec.gov/Archives/\"\n",
    "    total_num = len(df)\n",
    "    \n",
    "#     executor = concurrent.futures.ThreadPoolExecutor(max_workers=20)\n",
    "    cannot = []\n",
    "    for ind, rows in df.iterrows():\n",
    "        company = rows['CompanyName']\n",
    "        company = re.sub(r\"[\\\\+/+]\", \" \", company)\n",
    "        cik = rows['CIK']\n",
    "        date = rows['Date']\n",
    "        url = base_url + rows['FileName']\n",
    "        print(ind + 1, '/', total_num, ' : ', url)\n",
    "        filename = '--'.join([cik, company, date]) + '.txt'\n",
    "        \n",
    "        flag = True\n",
    "        times = 1\n",
    "        # try crawling three times if error occurs\n",
    "        while flag:\n",
    "            print(times)\n",
    "            if times < 3:\n",
    "                time.sleep(1)\n",
    "                try:\n",
    "                    download_processor(url,filename)\n",
    "#                     executor.submit(download_processor,url, filename)\n",
    "#                     print(\"Waiting for threads to finish...\")\n",
    "#                     executor.shutdown(wait=True)\n",
    "                    flag=False\n",
    "                except:\n",
    "                    times += 1\n",
    "            else:\n",
    "                flag=False\n",
    "                cannot.append(tuple([url, filename]))\n",
    "                \n",
    "\n",
    "directory = 'sec_10k'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# Get urls of 10-K filings in 2017\n",
    "idx_data = get_10k_path(2017)\n",
    "cannot = down_10k(idx_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current VER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import lxml\n",
    "import re\n",
    "import os\n",
    "import concurrent.futures\n",
    "import requests\n",
    "import os\n",
    "import errno\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "proxy_list = pd.read_table('proxy.txt', sep='\\t', header=-1)\n",
    "header_list = pd.read_table('header.txt', sep='\\t', header=-1)\n",
    "\n",
    "# DEFAULT_DATA_PATH = 'sec_10k/'\n",
    "# DEFAULT_DATA_PATH = 'C:/Users/Rocku/Downloads/sec_10k/'\n",
    "DEFAULT_DATA_PATH = 'D:\\seclogdown/'\n",
    "\n",
    "class SecCrawler():\n",
    "    ua = UserAgent()\n",
    "    def __init__(self):\n",
    "        print(\"Path of the directory: \" + DEFAULT_DATA_PATH)\n",
    "\n",
    "    def save_in_directory(self, company_code, cik, priorto, doc_list, doc_name_list, filing_type):\n",
    "        for j in range(len(doc_list)):\n",
    "            base_url = doc_list[j]\n",
    "            print(base_url)\n",
    "            r = requests.get(base_url, {'headers':self.ua.random})\n",
    "            report = soup(r.content.decode(), 'lxml')\n",
    "            report.find_all('table')\n",
    "            report = re.sub(r\"\\s\\s+\", \" \", report)\n",
    "            report = re.sub(r\"<.*?>\", \"\", report)\n",
    "            splitted = re.split(r'[iI]tem\\s+\\d+\\W+', report)\n",
    "            filename = '--'.join([company_code,cik,priorto,doc_name_list[j]])\n",
    "            flag_write = True\n",
    "            for chunk in splitted:\n",
    "                if (chunk.lower().strip().startswith('business') and len(chunk) > 500 and flag_write):\n",
    "                    with open(DEFAULT_DATA_PATH + filename, \"ab\") as f:\n",
    "                        f.write(chunk.encode('ascii', 'ignore'))\n",
    "                    flag_write = False\n",
    "                    print(filename, ' downloaded')\n",
    "            \n",
    "    def filing_10K(self, company_code, cik, priorto, count):\n",
    "\n",
    "        # generate the url to crawl\n",
    "        base_url = \"http://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=\"+str(cik)+\"&type=10-K&dateb=\"+str(priorto)+\"&owner=exclude&output=xml&count=\"+str(count)\n",
    "        r = requests.get(base_url)\n",
    "        data = r.text\n",
    "        doc_list, doc_name_list = self.create_document_list(data)\n",
    "\n",
    "        try:\n",
    "            self.save_in_directory(company_code, cik, priorto, doc_list, doc_name_list, '10-K')\n",
    "        except Exception as e:\n",
    "            print (str(e))\n",
    "\n",
    "\n",
    "    def create_document_list(self, data):\n",
    "        soup_obj = BeautifulSoup(data, 'lxml')\n",
    "        link_list = list()\n",
    "\n",
    "        # If the link is .htm convert it to .html\n",
    "        for link in soup_obj.find_all('filinghref'):\n",
    "            url = link.string\n",
    "            if link.string.split(\".\")[len(link.string.split(\".\"))-1] == \"htm\":\n",
    "                url += \"l\"\n",
    "            link_list.append(url)\n",
    "        link_list_final = link_list\n",
    "\n",
    "        doc_list = list()\n",
    "        doc_name_list = list()\n",
    "        for k in range(len(link_list_final[:1])):\n",
    "            required_url = link_list_final[k].replace('-index.html', '')\n",
    "            txtdoc = required_url + \".txt\"\n",
    "            docname = txtdoc.split(\"/\")[-1]\n",
    "            doc_list.append(txtdoc)\n",
    "            doc_name_list.append(docname)\n",
    "        return doc_list, doc_name_list\n",
    "    \n",
    "    \n",
    "# def reset_proxy():\n",
    "#     rand_sample = random.sample(range(len(proxy_list)), 1)\n",
    "#     rand_ip = proxy_list.ix[rand_sample][0].values\n",
    "#     rand_port = proxy_list.ix[rand_sample][1].values\n",
    "#     proxy_target = (str(*rand_ip) + ':' + str(*rand_port))\n",
    "#     return proxy_target\n",
    "\n",
    "\n",
    "# def reset_header():\n",
    "#     rand_sample = random.sample(range(len(header_list)), 1)\n",
    "#     rand_header = header_list.ix[rand_sample][0].values\n",
    "#     header_target = (str(*rand_header))\n",
    "#     return header_target\n",
    "\n",
    "\n",
    "def get_10k_path(year):\n",
    "    qtrs = ['QTR1', 'QTR2', 'QTR3', 'QTR4']\n",
    "    columns = ['CIK', 'CompanyName', 'FormType', 'Date', 'FileName']\n",
    "    base_url = \"http://www.sec.gov/Archives/edgar/full-index/%s/\" % year\n",
    "    df_base = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    for qtr in qtrs:\n",
    "        url = base_url + qtr + '/master.idx'\n",
    "        print(url)\n",
    "        lines = requests.get(url).text.splitlines()\n",
    "        records = [tuple(line.split('|')) for line in lines[11:]]\n",
    "        df = pd.DataFrame(records, columns=columns)\n",
    "        TenK = df[df['FormType'] == '10-K']\n",
    "        df_base = df_base.append(TenK)\n",
    "        del df, TenK, records\n",
    "    print('Total number of 10-K filings : ', len(df_base))\n",
    "    return df_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.sec.gov/Archives/edgar/full-index/2010/QTR1/master.idx\n",
      "http://www.sec.gov/Archives/edgar/full-index/2010/QTR2/master.idx\n",
      "http://www.sec.gov/Archives/edgar/full-index/2010/QTR3/master.idx\n",
      "http://www.sec.gov/Archives/edgar/full-index/2010/QTR4/master.idx\n",
      "Total number of 10-K filings :  9165\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(DEFAULT_DATA_PATH):\n",
    "    os.makedirs(DEFAULT_DATA_PATH)\n",
    "\n",
    "# Get urls of 10-K filings in 2017\n",
    "idx_data = get_10k_path(2010)\n",
    "idx_data.reset_index(inplace=True)\n",
    "idx_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>CIK</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>FormType</th>\n",
       "      <th>Date</th>\n",
       "      <th>FileName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>1000180</td>\n",
       "      <td>SANDISK CORP</td>\n",
       "      <td>10-K</td>\n",
       "      <td>2010-02-25</td>\n",
       "      <td>edgar/data/1000180/0001000180-10-000008.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79</td>\n",
       "      <td>1000209</td>\n",
       "      <td>MEDALLION FINANCIAL CORP</td>\n",
       "      <td>10-K</td>\n",
       "      <td>2010-03-12</td>\n",
       "      <td>edgar/data/1000209/0001193125-10-055581.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92</td>\n",
       "      <td>1000228</td>\n",
       "      <td>HENRY SCHEIN INC</td>\n",
       "      <td>10-K</td>\n",
       "      <td>2010-02-23</td>\n",
       "      <td>edgar/data/1000228/0001000228-10-000006.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>141</td>\n",
       "      <td>1000229</td>\n",
       "      <td>CORE LABORATORIES N V</td>\n",
       "      <td>10-K</td>\n",
       "      <td>2010-02-19</td>\n",
       "      <td>edgar/data/1000229/0001000229-10-000004.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>155</td>\n",
       "      <td>1000230</td>\n",
       "      <td>OPTICAL CABLE CORP</td>\n",
       "      <td>10-K</td>\n",
       "      <td>2010-01-29</td>\n",
       "      <td>edgar/data/1000230/0001193125-10-017031.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index      CIK               CompanyName FormType        Date  \\\n",
       "0     39  1000180              SANDISK CORP     10-K  2010-02-25   \n",
       "1     79  1000209  MEDALLION FINANCIAL CORP     10-K  2010-03-12   \n",
       "2     92  1000228          HENRY SCHEIN INC     10-K  2010-02-23   \n",
       "3    141  1000229     CORE LABORATORIES N V     10-K  2010-02-19   \n",
       "4    155  1000230        OPTICAL CABLE CORP     10-K  2010-01-29   \n",
       "\n",
       "                                      FileName  \n",
       "0  edgar/data/1000180/0001000180-10-000008.txt  \n",
       "1  edgar/data/1000209/0001193125-10-055581.txt  \n",
       "2  edgar/data/1000228/0001000228-10-000006.txt  \n",
       "3  edgar/data/1000229/0001000229-10-000004.txt  \n",
       "4  edgar/data/1000230/0001193125-10-017031.txt  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.sec.gov/Archives/edgar/data/1000228/0001000228-10-000006.txt\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://www.sec.gov/Archives/\"\n",
    "url = idx_data.loc[2,'FileName']\n",
    "url = base_url + url\n",
    "# print(url)\n",
    "ua = UserAgent()\n",
    "# url = 'https://www.sec.gov/Archives/edgar/data/1000209/0001193125-17-082178-index.htm'\n",
    "print(url)\n",
    "r = requests.get(url, {'headers':ua.random})\n",
    "report = soup(r.content.decode(), 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#item_1 Business\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'#item_1'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_business = ''\n",
    "break_flag = False\n",
    "for tab in report.find_all('table'):\n",
    "    if break_flag == True:\n",
    "        break\n",
    "    else:\n",
    "        for a_tag in tab.find_all('a'):\n",
    "            if a_tag is not None:\n",
    "                if 'business' in list(a_tag.text.lower().strip().split(' ')):\n",
    "                    print(a_tag['href'], a_tag.text)\n",
    "                    path_to_business = a_tag['href']\n",
    "                    break_flag = True\n",
    "                    break\n",
    "\n",
    "\n",
    "path_to_business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"#pt1_item_1_business\">Business</a>\n",
      "<a href=\"#pt1_item_1a_risk_factors\">Risk Factors</a>\n",
      "<a href=\"#pt1_item_1b_sec_comments\">Unresolved Staff\n",
      "    Comments</a>\n",
      "<a href=\"#pt1_item_2_property\">Properties</a>\n",
      "<a href=\"#item_3_legal_proceedings\">Legal Proceedings</a>\n",
      "<a href=\"#pt1_item_4_submissions\">Submission of Matters to a Vote of Security\n",
      "      Holders</a>\n",
      "<a href=\"#pt2_item_5_market_common_equity\">Market for Registrant’s Common\n",
      "      Equity, Related Stockholder Matters and Issuer Purchases of Equity\n",
      "      Securities</a>\n",
      "<a href=\"#pt2_item_6_financial_data\">Selected Financial\n",
      "    Data</a>\n",
      "<a href=\"#pt2_item_7_results_of_operations\">Management’s Discussion and\n",
      "      Analysis of Financial Condition and Results of\n",
      "    Operation</a>\n",
      "<a href=\"#pt2_item_7a_quantitative_disclosures\">Quantitative and Qualitative\n",
      "      Disclosures About Market Risk</a>\n",
      "<a href=\"#pt2_item_8_financial_statements\">Financial Statements and\n",
      "      Supplementary Data</a>\n",
      "<a href=\"#pt2_item_9_disagreements_w_accountants\">Changes in and\n",
      "      Disagreements with Accountants on Accounting and Financial\n",
      "      Disclosure</a>\n",
      "<a href=\"#pt2_item_9a_controls\">Controls and Procedures</a>\n",
      "<a href=\"#pt2_item_9b_other\">Other Information</a>\n",
      "<a href=\"#pt3_item_10_directors\">Directors, Executive Officers and Corporate\n",
      "      Governance</a>\n",
      "<a href=\"#pt3_item_11_exec_comp\">Executive Compensation</a>\n",
      "<a href=\"#pt3_item_12_security_ownership\">Security Ownership of Certain\n",
      "      Beneficial Owners and Management and Related Stockholder\n",
      "      Matters</a>\n",
      "<a href=\"#pt3_item_13_related_transactions\">Certain Relationships and Related\n",
      "      Transactions, and Director Independence</a>\n",
      "<a href=\"#pt3_item_14_principle_accounting\">Principal Accountant Fees and\n",
      "      Services</a>\n",
      "<a href=\"#pt4_item_15_exhibits\">Exhibits and Financial Statement\n",
      "      Schedules</a>\n",
      "<a href=\"#fsi\">Index To Financial Statements</a>\n",
      "<a href=\"#pt7_signatures\">Signatures</a>\n",
      "<a name=\"Pt1_Item_1_Business\">ITEM 1.</a>\n",
      "<a name=\"Pt1_Item_1A_Risk_Factors\">ITEM 1A.</a>\n",
      "<a name=\"Pt1_Item_1B_SEC_Comments\">ITEM 1B.</a>\n",
      "<a name=\"Pt1_Item_1B_SEC_Comments\">ITEM 2.</a>\n",
      "<a name=\"Pt1_Item_1B_SEC_Comments\">ITEM 3.</a>\n",
      "<a name=\"Pt1_Item_1B_SEC_Comments\">ITEM 4.</a>\n",
      "<a name=\"Pt2_Item_5_Market_Common_Equity\">ITEM 5.</a>\n",
      "<a name=\"Pt2_Item_6_Financial_Data\">ITEM 6.</a>\n",
      "<a name=\"Pt2_Item_7_Results_of_Operations_2004\"></a>\n",
      "<a name=\"Pt2_Item_7_Results_of_Operations\">ITEM 7.</a>\n",
      "<a name=\"Pt2_Item_7A_Quantitative_Disclosures\">ITEM 7A.</a>\n",
      "<a name=\"Pt2_Item_9_Disagreements_w_Accountants\">ITEM 9.</a>\n",
      "<a name=\"Pt2_Item_9A_Controls\">ITEM 9A.</a>\n",
      "<a name=\"Pt2_Item_9B_Other\">ITEM 9B.</a>\n",
      "<a name=\"Pt3_Item_10_Directors\">ITEM 10.</a>\n",
      "<a name=\"Pt3_Item_11_Exec_Comp\">ITEM 11.</a>\n",
      "<a name=\"Pt3_Item_12_Security_Ownership\">ITEM 12.</a>\n",
      "<a name=\"Pt3_Item_13_Related_Transactions\">ITEM 13.</a>\n",
      "<a name=\"Pt3_Item_14_Principle_Accounting\">ITEM 14.</a>\n",
      "<a name=\"Pt4_Item_15_Exhibits\">ITEM 15.</a>\n",
      "<a href=\"#pt5_item_f1_ey_reports\">Reports of Independent Registered Public\n",
      "      Accounting Firm</a>\n",
      "<a href=\"#pt5_item_f2_balance_sheet\">Consolidated Balance\n",
      "      Sheets</a>\n",
      "<a href=\"#pt2_item_7_results_of_operations\">Consolidated Statements of\n",
      "      Operations</a>\n",
      "<a href=\"#pt5_item_f4_equity\">Consolidated Statements of\n",
      "      Equity</a>\n",
      "<a href=\"#pt5_item_f5_cash_flow\">Consolidated Statements of Cash\n",
      "      Flows</a>\n",
      "<a href=\"#pt6_note1_organization\">Notes to Consolidated Financial\n",
      "      Statements</a>\n",
      "<a href=\"#pt5_item_f1_ey_reports\">Reports of Independent Registered Public\n",
      "      Accounting Firm</a>\n",
      "<a href=\"#pt5_item_f2_balance_sheet\">Consolidated Balance\n",
      "      Sheets</a>\n",
      "<a href=\"#pt5_item_f3_income_statement\">Consolidated Statements of\n",
      "      Operations</a>\n",
      "<a href=\"#pt5_item_f4_equity\">Consolidated Statements of\n",
      "      Equity</a>\n",
      "<a href=\"#pt5_item_f5_cash_flow\">Consolidated Statements of Cash\n",
      "      Flows</a>\n",
      "<a href=\"#pt6_note1_organization\">Notes to Consolidated Financial\n",
      "      Statements</a>\n",
      "<a name=\"option_foreiture_rate\">8.31%</a>\n",
      "<a name=\"fplguarantee\">538,693</a>\n",
      "<a name=\"falguarantee\">531,070</a>\n",
      "<a name=\"sndkguaranteeyen\">98.9</a>\n",
      "<a name=\"sndkguaranteeusd\">1,069,763</a>\n"
     ]
    }
   ],
   "source": [
    "for tab in report.find_all('table'):\n",
    "    for a_tag in tab.find_all('a'):\n",
    "        print(a_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path of the directory: D:\\seclogdown/\n",
      "http://www.sec.gov/Archives/edgar/data/1000209/000119312517082178/0001193125-17-082178.txt\n",
      "'SecCrawler' object has no attribute 'ua'\n",
      "http://www.sec.gov/Archives/edgar/data/1000228/000100022817000011/0001000228-17-000011.txt\n",
      "'SecCrawler' object has no attribute 'ua'\n",
      "http://www.sec.gov/Archives/edgar/data/1000229/000100022917000025/0001000229-17-000025.txt\n",
      "'SecCrawler' object has no attribute 'ua'\n"
     ]
    }
   ],
   "source": [
    "idx_data = idx_data.loc[:2,:]\n",
    "\n",
    "secCrawler = SecCrawler()\n",
    "for ind,ex in idx_data.iterrows():\n",
    "    CompanyName = re.sub(r\"\\W+\", \" \", ex['CompanyName']).strip()\n",
    "    CIK = ex['CIK']\n",
    "    Date = ex['Date'].replace('-','')\n",
    "    secCrawler.filing_10K(CompanyName, CIK, Date, '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
