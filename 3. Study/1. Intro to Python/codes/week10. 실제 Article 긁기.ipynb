{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import random\n",
    "import pandas as pd\n",
    "import xlsxwriter\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "proxy_list = pd.read_table('proxy2.txt',sep='\\t', header=-1)\n",
    "header_list = pd.read_table('header.txt',sep='\\t', header=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_proxy():\n",
    "    rand_sample = random.sample(range(len(proxy_list)), 1)\n",
    "\n",
    "    rand_ip = proxy_list.ix[rand_sample][0].values\n",
    "    rand_port = proxy_list.ix[rand_sample][1].values\n",
    "\n",
    "    proxy_target = (str(*rand_ip) +':'+  str(*rand_port))\n",
    "#     print(proxy_target)\n",
    "    \n",
    "    return proxy_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_header():\n",
    "    rand_sample = random.sample(range(len(header_list)), 1)\n",
    "\n",
    "    rand_header = header_list.ix[rand_sample][0].values\n",
    "\n",
    "    header_target = (str(*rand_header))\n",
    "#     print(header_target)\n",
    "    \n",
    "    return header_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url_data = pd.read_table('D:/Crawl/data/call_urls.txt', sep='\\t', header=-1)\n",
    "url_lists = list(url_data[0])\n",
    "print(url_lists[:5])\n",
    "print(len(url_lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_url = 'https://seekingalpha.com/article/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_logs(url_digits):\n",
    "    log_path = 'D:/Crawl/save_logs.txt'\n",
    "    \n",
    "    with open(log_path, 'a') as f:\n",
    "        f.write(str(url_digits) + ' : done ' + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_error_urls(h, p, url):\n",
    "    path = 'D:/Crawl/error_logs.txt'\n",
    "        \n",
    "    with open(path, 'a') as f:\n",
    "        f.write(h + ',' + p + ',' + url + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_panels(p_data):\n",
    "    exa_flag = False\n",
    "    ana_flag = False\n",
    "    \n",
    "    executives = []\n",
    "    analysts = []\n",
    "    \n",
    "    for i, p in enumerate(p_data):\n",
    "                    \n",
    "        if (p.strong is not None) and (p.text =='Executives'):\n",
    "            exa_flag = True\n",
    "            \n",
    "        if (p.strong is not None) and (p.text =='Analysts'):\n",
    "            exa_flag = False\n",
    "            ana_flag = True\n",
    "            \n",
    "        if (p.strong is not None) and (p.text != 'Analysts') and (p.text != 'Executives'):\n",
    "            exa_flag = False\n",
    "            ana_flag = False\n",
    "            return executives[1:], analysts[1:], i\n",
    "        \n",
    "        if exa_flag:\n",
    "            executives.append(p.string)\n",
    "            \n",
    "        if ana_flag:\n",
    "            analysts.append(p.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_body(p_data):\n",
    "    start_flag = False\n",
    "    concat_flag = False\n",
    "    \n",
    "    speakers = []\n",
    "    transcripts = []\n",
    "    \n",
    "    for i,p in enumerate(p_data):\n",
    "        \n",
    "        if (p.strong is not None) and (p.strong.a is None):\n",
    "            concat_flag = False\n",
    "            \n",
    "        if concat_flag:\n",
    "            transcripts[-1] = '\\n'.join([transcripts[-1], p.text])             \n",
    "\n",
    "        if start_flag:\n",
    "            transcripts.append(p.text)\n",
    "            start_flag = False\n",
    "            concat_flag = True\n",
    "            \n",
    "        if (p.strong is not None) and (p.strong.a is None):\n",
    "            speakers.append(p.text)\n",
    "            start_flag = True\n",
    "            concat_flag = False\n",
    "     \n",
    "    return speakers, transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_earnings(url):\n",
    "\n",
    "    headers = {'User-Agent': reset_header()}\n",
    "    proxy_dict = {'http': reset_proxy()}\n",
    "    \n",
    "    response = requests.get(url, headers=headers, proxies=proxy_dict)\n",
    "    data_dict = {}\n",
    "    \n",
    "    if (response.status_code == 200):\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        raw_data = soup.find('article')\n",
    "        header_data = raw_data.find('header')\n",
    "        data_dict['UploadDate'] = header_data.find('time').get('datetime')\n",
    "        data_dict['URL'] = header_data.find('meta').get('content')\n",
    "        data_dict['Title'] = header_data.find('h1').string\n",
    "        \n",
    "        body_data = raw_data.find('div', class_='article-width').findAll('p')\n",
    "        data_dict['Subtitle'] = body_data[1].text\n",
    "        data_dict['CallDate'] = body_data[2].text\n",
    "        \n",
    "        executives, analysts, start_point = get_panels(body_data)\n",
    "        data_dict['Executives'] = executives\n",
    "        data_dict['Analysts'] = analysts\n",
    "        \n",
    "        parsed_company = body_data[0].text.split('(')\n",
    "        \n",
    "        if len(parsed_company) != 1:\n",
    "            data_dict['CompanyName'] = parsed_company[0]\n",
    "            data_dict['Exchange'] = parsed_company[-1].split(':')[0]\n",
    "            data_dict['Ticker'] = parsed_company[-1].split(':')[1].replace(')', '')\n",
    "        else:\n",
    "            data_dict['CompanyName'] = parsed_company[0]\n",
    "            data_dict['Exchange'] = 'None'\n",
    "            data_dict['Ticker'] = 'None'\n",
    "        \n",
    "        speakers, transcripts = get_body(body_data[int(start_point):])\n",
    "        data_dict['Transcript'] = transcripts\n",
    "        data_dict['Speackers'] = speakers\n",
    "            \n",
    "        return data_dict\n",
    "    \n",
    "    else:\n",
    "        print(response.status_code, response.body)\n",
    "        save_error_urls(headers['User-Agent'], proxy_dict['http'], url)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def format_xls(url, data_dict):\n",
    "    root_path = 'D:/Crawl/'\n",
    "    path = root_path + 'SeekingAlpha_' + str(url) + '.xlsx'\n",
    "   \n",
    "    workbook = xlsxwriter.Workbook(path)\n",
    "    worksheet = workbook.add_worksheet('Sheet1') #시트 생성\n",
    "\n",
    "    start_row = 0\n",
    "    worksheet.write(start_row,0,'URL')\n",
    "    worksheet.write(start_row,1, data_dict['URL'])\n",
    "    \n",
    "    start_row += 1\n",
    "    worksheet.write(start_row,0,'Title')\n",
    "    worksheet.write(start_row,1, data_dict['Title'])\n",
    "    \n",
    "    start_row += 1        \n",
    "    worksheet.write(start_row,0,'UploadDate')\n",
    "    worksheet.write(start_row,1, data_dict['UploadDate'])\n",
    "    \n",
    "    start_row += 1     \n",
    "    worksheet.write(start_row,0,'CompanyName')\n",
    "    worksheet.write(start_row,1, data_dict['CompanyName'])\n",
    "    \n",
    "    start_row += 1     \n",
    "    worksheet.write(start_row,0,'Exchange')\n",
    "    worksheet.write(start_row,1, data_dict['Exchange'])\n",
    "    \n",
    "    start_row += 1     \n",
    "    worksheet.write(start_row,0,'Ticker')\n",
    "    worksheet.write(start_row,1, data_dict['Ticker'])\n",
    "    \n",
    "    start_row += 1     \n",
    "    worksheet.write(start_row,0,'Subtitle')\n",
    "    worksheet.write(start_row,1, data_dict['Subtitle'])\n",
    "    \n",
    "    start_row += 1     \n",
    "    worksheet.write(start_row,0,'CallDate')\n",
    "    worksheet.write(start_row,1, data_dict['CallDate'])\n",
    "    \n",
    "    last_row = start_row+1\n",
    "    \n",
    "    num_ex = len(data_dict['Executives'])\n",
    "    num_an = len(data_dict['Analysts'])\n",
    "    num_tr = len(data_dict['Transcript'])\n",
    "#     print(num_ex, num_an, num_tr)\n",
    "    \n",
    "    if (num_ex > 0) :\n",
    "        for i in range(num_ex):\n",
    "            worksheet.write(last_row,0, 'Executives')\n",
    "            worksheet.write(last_row,1, data_dict['Executives'][i])\n",
    "#             worksheet.write(last_row,1, data_dict['Executives'][i].split(' - ')[0])\n",
    "#             worksheet.write(last_row,2, data_dict['Executives'][i].split(' - ')[1])\n",
    "            last_row += 1\n",
    "    \n",
    "    if (num_an > 0) :\n",
    "        for i in range(num_an):\n",
    "            worksheet.write(last_row,0, 'Analysts')\n",
    "            worksheet.write(last_row,1, data_dict['Analysts'][i])\n",
    "#             worksheet.write(last_row,1, data_dict['Analysts'][i].split(' - ')[0])\n",
    "#             worksheet.write(last_row,2, data_dict['Analysts'][i].split(' - ')[1])\n",
    "            last_row += 1\n",
    "    \n",
    "    for i in range(num_tr-1):\n",
    "        worksheet.write(last_row,0, 'Transcript')\n",
    "        worksheet.write(last_row,1, data_dict['Speackers'][i])\n",
    "        \n",
    "        if data_dict['Speackers'][i].startswith('Question-'):\n",
    "            pass\n",
    "        else:\n",
    "            worksheet.write(last_row,2, data_dict['Transcript'][i])\n",
    "        last_row += 1\n",
    "\n",
    "    workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('----------start-------------')\n",
    "\n",
    "for i, url_digit in enumerate(url_lists[:10]):\n",
    "    \n",
    "    if (i%50 == 0):\n",
    "        print(i, end=' ')\n",
    "    \n",
    "    url = base_url + str(url_digit)\n",
    "\n",
    "    try:\n",
    "        data_dict = get_earnings(url)\n",
    "        if (data_dict is not None):\n",
    "            format_xls(url_digit, data_dict)\n",
    "            save_logs(url_digit)\n",
    "    except:\n",
    "        try:\n",
    "            data_dict = get_earnings(url)\n",
    "            if (data_dict is not None):\n",
    "                format_xls(url_digit, data_dict)\n",
    "                save_logs(url_digit)\n",
    "        except:\n",
    "            e = sys.exc_info()[1]\n",
    "            save_error_urls(str(e), '::', url.split('/')[-1])\n",
    "            \n",
    "    randtime = 0.5 + round(random.random(),2)\n",
    "    time.sleep(randtime)\n",
    "\n",
    "print('\\n----------end-------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missed = []\n",
    "\n",
    "with open('D:/Crawl/last/error_logs.txt', 'r') as f:\n",
    "    missing = f.readlines()\n",
    "    for lines in missing:\n",
    "        if lines.startswith(',,'):\n",
    "            missed.append(int(lines.replace('\\n','').split(',,')[-1]))\n",
    "        else:\n",
    "            missed.append(int(lines.replace('\\n','').split(',')[-1]))    \n",
    "        \n",
    "print(missed[:5])\n",
    "print(len(missed))\n",
    "\n",
    "missed = list(set(missed))\n",
    "print(len(missed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('----------start-------------')\n",
    "\n",
    "for i, url_digit in enumerate(missed):\n",
    "    \n",
    "    if (i%50 == 0):\n",
    "        print(i, end=' ')\n",
    "#     print(url_digit, end=' ')\n",
    "    url = base_url + str(url_digit)  \n",
    "    \n",
    "    try:\n",
    "        data_dict = get_earnings(url)\n",
    "        if (data_dict is not None):\n",
    "            format_xls(url_digit, data_dict)\n",
    "            save_logs(url_digit)\n",
    "    except:\n",
    "        try:\n",
    "            data_dict = get_earnings(url)\n",
    "            if (data_dict is not None):\n",
    "                format_xls(url_digit, data_dict)\n",
    "                save_logs(url_digit)\n",
    "        except:\n",
    "            e = sys.exc_info()[1]\n",
    "            save_error_urls(str(e), '::', url.split('/')[-1])\n",
    " \n",
    "    randtime = 0.5 + round(random.random(),2)\n",
    "    time.sleep(randtime)\n",
    "\n",
    "print('\\n----------end-------------')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
